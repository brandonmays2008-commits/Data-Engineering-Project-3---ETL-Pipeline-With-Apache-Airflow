# Data Engineering Project 3 â€” ETL Pipeline with Apache Airflow

## ğŸ§  Overview
This project demonstrates a simple ETL (Extract, Transform, Load) pipeline using **Apache Airflow** on **Google Cloud Composer**. The pipeline extracts data from a CSV file, performs column transformations, and loads it into a simulated target database.

---

## âš™ï¸ ETL Pipeline Flow

1. **Extract** â€“ Reads a CSV file (`netflix_titles.csv`) stored in a Google Cloud Storage bucket.  
2. **Transform** â€“ Cleans and standardizes column formats (e.g., converts text to lowercase, formats dates).  
3. **Load** â€“ Simulates loading the cleaned data into a target table.

---

## ğŸ“‚ DAG Configuration
- **DAG Name:** `simple_etl_pipeline`  
- **Schedule:** `@daily`  
- **Owner:** `airflow`  
- **Environment:** `Google Cloud Composer`  
- **Tools Used:** Python, Airflow, GCP  

---

## ğŸ§© Key Components
- **Airflow DAG:** Defines the pipeline tasks and dependencies.  
- **Google Cloud Storage:** Hosts the CSV file.  
- **Cloud Composer:** Manages orchestration and monitoring.  

---

## ğŸ§± Tasks
| Task | Description | Status |
|------|--------------|---------|
| Extract Data | Reads data from CSV | âœ… Success |
| Transform Data | Cleans and formats columns | âœ… Success |
| Load Data | Simulates data load | âœ… Success |

---

## ğŸ“Š Results

### âœ… DAG Deployment
![Descriptive Alt Text](figure1.png)

### âœ… DAG Visibility
![Descriptive Alt Text](figure2.png)

### âœ… DAG Execution Logs
![Descriptive Alt Text](figure3.png)

All tasks (`extract_data`, `transform_data`, and `load_data`) completed successfully, indicating the pipeline executed end-to-end without errors.

---

## ğŸš€ Takeaways
- Successfully deployed a working **ETL pipeline** on **Google Cloud Composer (Airflow)**.  
- Demonstrated orchestration of data extraction, transformation, and loading tasks.  
- Validated understanding of Airflow DAG structure, task dependencies, and environment setup.  

